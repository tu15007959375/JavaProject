# 学习日记-mc64（10）
## 本周进度
### mpi并行
* 221节点只有一张cpu，无法mpi并行，决定移植到25节点上，在25节点先测性能
* 单线程O2优化(60S)
![25+O2](/mymd/学习日记-mc64（10）/25+O2.png)
* 8线程O2(15S)
![25+O2+8](/mymd/学习日记-mc64（10）/25+O2+8.png)
* 16线程O2(10S)
![25+O2+16](/mymd/学习日记-mc64（10）/25+O2+16.png)
* 32线程O2(7s)
![25+O2+32](/mymd/学习日记-mc64（10）/25+O2+32.png)
### 11.15
* step3初始版本，考虑将寻找增广路的过程分给多进程，首先统计未匹配的节点数fc，均匀将未匹配节点分配给每个进程，然后一轮寻找完后，其他进程将自己寻找到的路线数据，即ufr（寻找到的没有匹配节点的节点），fr（寻找到有匹配节点的节点），rparent（两个节点之前所连边的索引）、rroot（最初开始寻找增广路的节点索引，即最开始的fc）发送给0号进程进行合并。
* 合并问题存在如下：之前是openmp版本，所以每次找到节点后会立即更新rparent，这样即使是多线程也不会存在两个未匹配的节点找到同一个节点的情况。但是mpi之后，因为拆分了任务量，每个进程的rparent，fr，ufr各自独立，存在找到同一个节点的情况，所以需要考虑解决方法
* 初步设想的解决办法如下:在每个进程找到相应路线和节点后，数据均发给0号进程进行处理，对于fr、ufr，进行取并集操作即可，但是对于rparent和rroot，如每个节点相同位置可能值不一样，即找到了同一个节点的情况，考虑放弃其中一个未匹配fc找到的节点，初步规则是选择rroot值较大的，因为还存在一个进程对于fc节点找到了相应节点r，但是其他进程没有找到的r的情况，这种情况就需要取找到了情况。如果取较大值就囊括了这种情况，无需做二次判断
* 效果：初步的效果非常差，之前单机只需要3s的step3对于mpi 4进程增加到了20s，下一步决定测试主要耗时在哪
### 11.16
* 11.16：对step3进行mpi耗时测试，发现其中15s都在rparent和rroot值的接收上，初步猜测是它们的大小均为m，即矩阵行列大小，每次都是传输如此大的数据，并且还需要十余次，所以耗时很大。
* 解决办法1（**不可行**）：决定使用mpi_gather函数进行一次性的函数收集，并且在0号进程进行多线程处理，寻找最大的数并且赋值，但是多次测试后，发现程序卡死，并且发现卡在接收数据时，应该是数据太大导致接收很慢，所以此方法放弃
* 解决办法2（**不可行**）：去查看论文，寻找解决办法，论文中对于这两个数组是使用pair实现的，我们的代码是使用固定大小的数组实现，并且其实每次寻找节点其实大部分索引都没有用到，所以才导致传输了很多无用数据，并且还进行了无效的比较。所以打算使用map来进行存储，但是mpi对于c++的stl对象传输并不支持，只有转成结构体对象或者使用boost库去实现，考虑到困难程度，决定放弃
* 解决办法3（**可行**）：想到使用数组实现，并且每三个为一组，第一个数为节点索引，二三依次为它的rparent和rroot，这样就不会传输困难并且传输的也都是有用数据，实现后，step3的时间降低到了6s左右，耗时减少，但是还是没有单机的快，所以需要再考虑优化
### 11.17
* 考虑优化方向1：将数据传输发送时改为异步
* 考虑优化方向2：接收时不能以for循环来一个一个来接收数据，这样可能造成其他进程数据早就发了，但是进程0只需要指定线程的数据。
* 考虑优化方向3：step3的mpi只是实现了其中的大头，考虑看看其他的部分还能不能上mpi加快速度
* 解决办法1（**不可行**）对fr和ufropenmp合并的时候，进行多线程合并，即先把每个线程的数组大小记录，然后进行累加计算出偏移地址，然后进行多线程合并，但是效果不大，决定放弃
* 解决办法2（**可行**）：将数据传输改为MPI_Gatherv方法，并且对于fr和ufr只进行合并，因为经过测试重复的元素并不多，如果去重会消耗些许时间，时间减少到了5s左右
### 11.18
* 对step4进行mpi，主要传输的数据为cmate和rmate，即匹配结果，以及寻找到的四个可以交换的节点。但是节点在代码中是使用`vector<tuple<double, int, int, int, int>> S`存储，根本无法传输数据，所以决定改为vector或者数组存储
* 继续使用**MPI_Gatherv**代替**MPI_Send**和**MPI_Recv**,将节点n分成进程等分，最后进程0将寻找到的节点进行收集并且进行节点交换，最后将匹配mate信息广播给其他进程，进行下一轮循环
* 效果：目前测试的矩阵omp版本为4s，4进程mpi为2s，加速比为2x
* 改进想法：考虑对收集到的信息进行先处理再交换，因为里面有重复的节点的话，是可以去除的
### 总结
* step2和step5耗时很少，如果使用mpi，可能导致通信时间大于计算时间，所以没加mpi
* step1是读矩阵，目前只是单机版，因为读矩阵应该不考虑计时
* step3的mpi实现效果，4个进程效果和单机的差不多，需要后续再进行通信优化减少时间
* step4的初步mpi效果为，4进程达到了2x的加速比，还有优化空间
### 存在问题
* 对于读取文件方面，目前还是单机读取，后续需要更改为openmp和mpi读取
* 对主要耗时进行mpi后，耗时并没有减少，反而增加了一些，测试得知是通信耗时，后续还需要减少通信时间和减少通信的数据量
* 在openmp版本，经过测试，其他矩阵数据的耗时在step3有明显增加，发现是寻找增广路减少节点速度过慢，应该是step3的逻辑有问题，需要排查
